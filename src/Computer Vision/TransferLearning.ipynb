{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "In this task we will show how to perfom transfer learning - one of the most important things in image processing - with PyTorch. \n",
    "\n",
    "Transfer Learning describes the process of using an already trained network and retraining only a few layers in order to match our specific task.\n",
    "\n",
    "We previously learned that the convolutional layers in our model are used for feature extraction. This means we can take one of the best performing model (eg. VGG-19) and use their pretrained convolutional layers in order to extract the features of our images and then we retrain the fully connected layers at the end of the network to match our fruit classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23d859d2b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torchvision\n",
    "\n",
    "# The file helper_functions.py includes functions which perform the steps done in exercise 1\n",
    "#from helper_functions import create_dataloader, train_validate_model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim \n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data\n",
    "First we give you a short overview of the structure of the data, so that you have all the necessary information for building a DataLoader and your first model.\n",
    "\n",
    "Folder Structure as follows:\n",
    "\n",
    "* data\n",
    "    * class1\n",
    "        * image1\n",
    "        * image2\n",
    "\n",
    "    * class2\n",
    "        * image3\n",
    "        * image4\n",
    "\n",
    "    * ...\n",
    "\n",
    "\n",
    "First we get all folder names in the 'data' folder and save them as our class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datafolder = '..\\\\data\\\\frames'\n",
    "datafolder = 'C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\data\\\\frames'\n",
    "#             'C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\data\\\\frames'\n",
    "classes = os.listdir(datafolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit a Label Encoder to transform our class names to integers and then create a dataframe which contains all image paths and thei respective class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path label\n",
       "0  C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...     0\n",
       "1  C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...     0\n",
       "2  C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...     0\n",
       "3  C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...     0\n",
       "4  C:\\Users\\cwimmer\\OneDrive - Capgemini\\Projekte...     0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder().fit(classes)\n",
    "\n",
    "data_df = pd.DataFrame(columns=['image_path', 'label'])\n",
    "\n",
    "for class_ in classes:\n",
    "    for image in os.listdir(datafolder + '\\\\' + class_):\n",
    "        row = {'image_path': datafolder + '\\\\' + class_+ '\\\\' + image, 'label':label_encoder.transform([class_])[0]}\n",
    "        data_df = data_df.append(row, ignore_index=True)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\data\\\\frames\\\\kermit\\\\Muppets-02-01-01_f1000.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shuffle our dataset and then split it in a train, validation and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train-Examples>>> 1488\n",
      "# Val-Examples \t>>> 372\n",
      "# Test-Examples >>> 466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = shuffle(data_df, random_state=1)\n",
    "train_val_df, test_df = train_test_split(data_df, random_state=1, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_val_df, random_state=1, test_size=0.2)\n",
    "print('# Train-Examples>>> {}\\n# Val-Examples \\t>>> {}\\n# Test-Examples >>> {}\\n'.format(len(train_df), len(val_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Dataset\n",
    "\n",
    "This Dataset Class should inhert the torch.utils.data.Dataset class and overwrite the __init__, __len__ and __getitem__ methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_loader_df, transform):\n",
    "        self.name_frame = data_loader_df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''Return the length of the dataframe as an integer'''\n",
    "        length = len(self.name_frame)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        1. Get the path of the image with the index\n",
    "        2. open the Image with the PIL Library\n",
    "        3. transform the image with the transformer\n",
    "        4. get the label of the image witht the index\n",
    "        5. retrun the sample as a tuple\n",
    "        '''\n",
    "        img_name = self.name_frame.iloc[idx].image_path\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = self.name_frame.iloc[idx].label\n",
    "        sample = (image, label)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer object is accessible under imageTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_size = 260 # smallest side of image resized to 255px\n",
    "output_size = 256 # image cropped so it has size 244x244px\n",
    "\n",
    "imageTransformer = transforms.Compose([\n",
    "    transforms.Resize((resize_size)),\n",
    "    transforms.CenterCrop(output_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the PyTorch Datasets\n",
    "\n",
    "Now we use our implemented class to create the train, test and validation dataset as PyTorch datasets.\n",
    "\n",
    "_Side Note: The test set is not used in this Example but it is common to validate and compare different models with the validation set and then test the best model on this unused test set. This is done to avoid overfitting a model only to the validation set which does not generalize to unseen data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = imageDataset(train_df, transform=imageTransformer)\n",
    "test_dataset = imageDataset(test_df, transform=imageTransformer)\n",
    "val_dataset = imageDataset(val_df, transform=imageTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our datasets by printing the shape of the first sample image\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of our images is: (channels, height, width) with channels being the red, green and blue channel in RGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch Dataloader\n",
    "\n",
    "Next we want to create Dataloader Objects for out datasets which can be used to train our model. This Dataloader object should be used in that way:\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    model.train(images, labels)\n",
    "    \n",
    "\n",
    "* images as tensor with shape (batch_size, channels, height, width)\n",
    "* labels as tensor with shape (batch_size)\n",
    "\n",
    "\n",
    "Therefor we use the torch.utils.data.DataLoader class to transform our datasets in dataloader. But first we have to define the batch size to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worker paramter defines how many python subprocesses should be launched to load the data while training. This is especially usefull when training with a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# test our dataloader by printing the shape of the first batch\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained VGG-19 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "vgg19 = torchvision.models.vgg19(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print the model to see all layers\n",
    "print(vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Output above shows the model architecture of the VGG-19 Model. We can see that the layers are groups in 3 Groups. The first group consists of Convolutional and Pooling Layers and is used to extract features (like the name suggests). The second group is a single Average Pooling Layer. The thrid and last layer is used to classify the images in the respective groups.\n",
    "\n",
    "There are 3 fully connected Layers whereas the last the output layer is. We can see, that the last layer has 1000 Out-Features - this means that this model was trained to classify 1000 different classes. However, over task is only to classify between kirmet and not kirmet.\n",
    "\n",
    "Therefore we have to change the last layer to only output 2 different out_features. This is done by getting the in_features (we could also just read them from above) and then changing the last layer with a new layer which has the same in_features but only out 2 out_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the last layer to match class size\n",
    "in_features = vgg19.classifier[6].in_features\n",
    "vgg19.classifier[6] = nn.Linear(in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our network architecture fits our task we only have one last thing to do.\n",
    "\n",
    "We have to 'freeze' the first two layer groups. This is done by telling the parameters of the layers that they do not have to calculate the gradients during training. This is done by setting the requires_grad parameter to False. \n",
    "\n",
    "We now iterate over every group and then over every layer and set the requires_grad paramter to False except the group is calles classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features has been frozen.\n",
      "avgpool has been frozen.\n",
      "classifier has been unfrozen.\n"
     ]
    }
   ],
   "source": [
    "for name, child in vgg19.named_children():\n",
    "    if name in ['classifier']:\n",
    "        print(name + ' has been unfrozen.')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(name + ' has been frozen.')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is ready to be trained. If we now call our training function only the layers in the classifer group will be trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparamters\n",
    "\n",
    "In this task we define 2 very important Hyperparamters for out training.\n",
    "\n",
    "1. The Optimizer Function (in torch.optim module)\n",
    "2. The Loss Criterion (in torch.nn module)\n",
    "\n",
    "We use the Adam Optimizer and the Cross Entropy Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Hyperparamters (same as previous exercises)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=3e-4)\n",
    "epochs = 10\n",
    "\n",
    "vgg19 = vgg19.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(model, model_name, train_loader, val_loader, optimizer, criterion, epochs, device=torch.device('cpu')):\n",
    "    \n",
    "    case_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} ...'.format(epoch+1))\n",
    "        \n",
    "        ### Train\n",
    "            # initialize metrics for training\n",
    "        train_loss = train_accuracy = counter = 0\n",
    "        # for-loop \n",
    "        for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Clear optimizer\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                output = model.forward(inputs)\n",
    "                # Loss\n",
    "                loss = criterion(output, labels)\n",
    "                # calculate gradients (backpropogation)\n",
    "                loss.backward()\n",
    "                # Adjust parameters based on gradients\n",
    "                optimizer.step()\n",
    "                # Add the loss to the training set's running loss\n",
    "                train_loss += loss.item()*inputs.size(0)\n",
    "                # Calcuate Train_Accuracy\n",
    "                top_p, top_class = output.topk(1, dim=1)\n",
    "                # See how many of the classes were correct?\n",
    "                equals = top_class == labels.view(*top_class.shape)            \n",
    "                # Calculate the mean (get the accuracy for this batch)\n",
    "                # and add it to the running accuracy for this epoch\n",
    "                train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "\n",
    "                # Print the progress of our training\n",
    "                counter += 1\n",
    "                print('{}{}{} Train-Batches Progressed\\r'.format(counter, \"/\", len(train_loader)), end=\"\")\n",
    "\n",
    "        train_loss = round(train_loss/len(train_loader.dataset), 4)\n",
    "        train_accuracy = round(train_accuracy/len(train_loader), 4)\n",
    "        print('Training Loss: {} \\t\\tTraining Accuracy: {}'.format(train_loss, train_accuracy))\n",
    " \n",
    "        # save model\n",
    "#        torch.save(model.state_dict(), 'C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\src\\\\Computer Vision\\\\models\\\\transfer_learning\\\\{}-epoch-{}-val_acc-{}.pt'.format(model_name,(epoch+1), val_accuracy))\n",
    "   \n",
    "        \n",
    "        \n",
    "        ### Validation\n",
    "        # initialize metrics for every epoch\n",
    "        val_loss = val_accuracy = counter = 0\n",
    "        \n",
    "        y_true = torch.tensor([], dtype=torch.long, device=device)\n",
    "        all_outputs = torch.tensor([], device=device)\n",
    "        \n",
    "        \n",
    "        # Tell torch not to calculate gradients\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:  \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass\n",
    "                output = model.forward(inputs)            \n",
    "                # Calculate Loss\n",
    "                valloss = criterion(output, labels)            \n",
    "                # Add loss to the validation set's running loss\n",
    "                val_loss += valloss.item()*inputs.size(0)\n",
    "                # Since our model outputs a LogSoftmax, find the real           \n",
    "                # Get the top class of the output\n",
    "                top_p, top_class = output.topk(1, dim=1)\n",
    "\n",
    "\n",
    "                # See how many of the classes were correct?\n",
    "                equals = top_class == labels.view(*top_class.shape)            \n",
    "                # Calculate the mean (get the accuracy for this batch)\n",
    "                # and add it to the running accuracy for this epoch\n",
    "                val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                # Print the progress of our evaluation\n",
    "                counter += 1\n",
    "                \n",
    "                # catinate all true labels\n",
    "                y_true = torch.cat((y_true, labels), 0)\n",
    "                # catinate all predicted labes\n",
    "                all_outputs = torch.cat((all_outputs, output), 0)\n",
    "                \n",
    "                \n",
    "                print('{}{}{} Validation-Batches Progressed\\r'.format(counter, \"/\", len(val_loader)), end=\"\")\n",
    "        \n",
    "        ### store the metrics for graphs, confusion matrix, ROC and AUC for later\n",
    "        # y_true to numpy\n",
    "        y_true = y_true.cpu().numpy()  \n",
    "        # y_pred to numpy (all_outputs)\n",
    "        _, y_pred = torch.max(all_outputs, 1)\n",
    "        \n",
    "        # y_pred to numpy (all_outputs)\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        # get prediction probability of all outputs to numpy\n",
    "        y_pred_prob = F.softmax(all_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "   \n",
    "        \n",
    "        # Get the average loss for the entire epoch\n",
    "        val_loss = round(val_loss/len(val_loader.dataset), 4)\n",
    "        val_accuracy = round(val_accuracy/len(val_loader), 4)\n",
    "        # Print out the information\n",
    "        print('Validation Loss:  {} \\tValidation Accuracy: {}'.format(val_loss, val_accuracy))\n",
    "        \n",
    "        # save model\n",
    "        torch.save(model.state_dict(), 'C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\src\\\\Computer Vision\\\\models\\\\transfer_learning\\\\{}-epoch-{}-val_acc-{}.pt'.format(model_name,(epoch+1), val_accuracy))\n",
    "\n",
    "        # save every metric in a dict\n",
    "        case = {'train_accuracy': train_accuracy, 'train_loss': train_loss,\n",
    "                                  'val_accuracy': val_accuracy, 'val_loss': val_loss, 'y_true' : y_true,\n",
    "                                  'y_pred' : y_pred, 'y_pred_prob' :y_pred_prob}\n",
    "        \n",
    "        \n",
    "        # append each metric dict per epoch\n",
    "        case_list.append(case)\n",
    "        \n",
    "\n",
    "        print('-------------')\n",
    "    return case_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kirmet_detection = 'kirmet_detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ...\n",
      "Training Loss: 0.2281 \t\tTraining Accuracy: 0.9355\n",
      "Validation Loss:  0.6392 \tValidation Accuracy: 0.8688\n",
      "-------------\n",
      "Epoch 2 ...\n",
      "Training Loss: 0.1277 \t\tTraining Accuracy: 0.9574\n",
      "Validation Loss:  1.1085 \tValidation Accuracy: 0.8745\n",
      "-------------\n",
      "Epoch 3 ...\n",
      "43/47 Train-Batches Progressed\r"
     ]
    }
   ],
   "source": [
    "result = train_validate_model(vgg19, kirmet_detection, train_loader, val_loader, optimizer, criterion, epochs, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df(results_from_model):\n",
    "    df = pd.DataFrame(results_from_model[1])\n",
    "    df = df.drop(['y_true', 'y_pred', 'y_pred_prob'], axis=1)\n",
    "#    df['val_accuracy'] = df['val_accuracy'].div(12)\n",
    "#    df['val_loss'] = df['val_loss'].div(372)\n",
    "    df['epoch'] = df.index + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_metrics_df(result)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Training Curve\") \n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.plot(df['epoch'], df['train_accuracy'], label = 'train_accuracy')\n",
    "plt.plot(df['epoch'], df['train_loss'], label = 'train_loss')\n",
    "plt.plot(df['epoch'], df['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(df['epoch'], df['val_loss'], label = 'val_loss')\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left', scatterpoints = 1, frameon=False)\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\src\\\\Computer Vision\\\\plots\\\\Trainingcurve_TL.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Network to reset it\n",
    "model = vgg19(output_size, len(classes)).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\src\\\\Computer Vision\\\\models\\\\model-7.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,test_loader,criterion, device=torch.device('cpu')):\n",
    "        # initialize metrics for every epoch\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0 \n",
    "        counter = 0\n",
    "        \n",
    "        y_true = torch.tensor([], dtype=torch.long, device=device)\n",
    "        all_outputs = torch.tensor([], device=device)\n",
    "        \n",
    "        \n",
    "        # Tell torch not to calculate gradients\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:  \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass\n",
    "                output = model.forward(inputs)            \n",
    "                # Calculate Loss\n",
    "                testloss = criterion(output, labels)            \n",
    "                # Add loss to the validation set's running loss\n",
    "                test_loss += testloss.item()*inputs.size(0)\n",
    "                # Since our model outputs a LogSoftmax, find the real           \n",
    "                # Get the top class of the output\n",
    "                top_p, top_class = output.topk(1, dim=1)\n",
    "\n",
    "\n",
    "                # See how many of the classes were correct?\n",
    "                equals = top_class == labels.view(*top_class.shape)            \n",
    "                # Calculate the mean (get the accuracy for this batch)\n",
    "                # and add it to the running accuracy for this epoch\n",
    "                test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                # Print the progress of our evaluation\n",
    "                counter += 1\n",
    "\n",
    "                #outputs = model(*inputs)\n",
    "                y_true = torch.cat((y_true, labels), 0)\n",
    "                all_outputs = torch.cat((all_outputs, output), 0)\n",
    "                \n",
    "                \n",
    "                print('{}{}{} Test-Batches Progressed\\r'.format(counter, \"/\", len(test_loader)), end=\"\")\n",
    "                             \n",
    "        \n",
    "        y_true = y_true.cpu().numpy()  \n",
    "        _, y_pred = torch.max(all_outputs, 1)\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_pred_prob = F.softmax(all_outputs, dim=1).cpu().numpy()\n",
    "                \n",
    "        # Get the average loss for the entire epoch\n",
    "        test_loss = round(test_loss/len(test_loader.dataset), 4)\n",
    "        test_accuracy = round(test_accuracy/len(test_loader), 4)\n",
    "        # Print out the information\n",
    "        print('Test Loss:  {} \\tTest Accuracy: {}'.format(test_loss, test_accuracy))\n",
    "        return test_loss, test_accuracy, y_true, y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test_model(model,test_loader,criterion, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC, Confusion Matrix, Accuracy, Recall, F1-Score with test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_pred_prob = test_metrics[4][:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_metrics[2], y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic of kirmet classifier')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "plt.savefig('C:\\\\Users\\\\cwimmer\\\\OneDrive - Capgemini\\\\Projekte\\\\Similarity Modelling 2\\\\src\\\\Computer Vision\\\\plots\\\\ROC_TL.png')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['kirmet', 'no kirmet']\n",
    "print(classification_report(y_true = test_metrics[2], y_pred = test_metrics[3], target_names = CATEGORIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_true=test_metrics[2], y_pred=test_metrics[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm           = confusion_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = CATEGORIES,\n",
    "                      title        = \"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
